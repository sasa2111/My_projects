# Dag code for airflow

В даге описан процесс обработки того же самого датасета с автомобилями, что и в других примерах.

Для запуска дага был развернут локально Airflow в виде нескольких докер-контейнеров (воркер, планировщик, бд, и тд), организованных с помощью уже готового docker-compose.yaml

Разворачивание всей этой структуры сделано мною по учебным инструкциям, которые предоставили в рамках курса обучения.


Что требовалось сделать в рамках задания:

1. написать модуль для импорта в файл c дагом (predict.py) - этот модель берет сериализованную модель (pkl) и делает предикты на тестовых данных (предсказывает ценовую категорию авто), сохраненных в виде файлов json в отдельной папке, сохраняет результаты предиктов в единый датафрейм - и складывает в виде файла csv в указанную папку.

2. написать даг, состоящий из 2 шагов: pipeline - предобработка датасета и моделирование, predict - предсказания ценовых категорий на тестовой выборке.

3. обеспечить, чтобы на локальном хосте 8080 открывался Airflow и отображался написанный мною Dag.

4. Обеспечить, чтобы даг мог достучаться до датасета и тестовых данных, лежащих в файле на компьютере, и отработал весь пайплайн без ошибок.

5. Обеспечить, чтобы по итогам отработки дага в соответствующих папках появилась сохраненная модель и результаты предсказаний для тестовых данных были сохранены в нужных папках. 

Что было сделано в рамках задания:
1. написан hw_dag.py для Airflow
2. Написан модуль predict.py 
(второй модуль pipeline.py взят уже готовый, его не переписывала).
3. Дополнен файл docker-compose.yml в разделе volumes - внесена 1 строка для возможности контейнерам видеть нужные файлы и папку в проекте.
4. Достигнута работоспособность всей этой системы. Даг отработал, составил модель, сделал предикты, собрал их в датафрейм (и сохранил их в файл). Сохраненные данные имеют следующий вид:

,obj,prediction

0,7310993818.json,['low']

1,7313922964.json,['high']

2,7315173150.json,['low']

3,7316152972.json,['medium']

4,7316509996.json,['high']
